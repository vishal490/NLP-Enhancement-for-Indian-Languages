{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhbPJujqSEM-"
   },
   "source": [
    "### Installing All The Dependency"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lCr-F7VbTdpY",
    "outputId": "0d1a7a44-ac4d-4bca-e168-ba2eaee75e52"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35uhORg5cx0E",
    "outputId": "4d07670d-234f-4d9b-b775-412494867880"
   },
   "source": [
    "%cd /content/drive/MyDrive"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dm9z-8m1SENE",
    "outputId": "a2ea86a0-9954-46d4-9858-17e3f80b39f8"
   },
   "source": [
    "!git clone https://github.com/VarunGumma/IndicTransTokenizer "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IBP-avMNdl3_",
    "outputId": "dc29e5e6-882d-4d53-de88-d56823b90017"
   },
   "source": [
    "%cd /content/drive/MyDrive/IndicTransTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "MVEEapo1eHOx",
    "outputId": "cfce2b5e-ab70-4ced-d0e8-dc9297217bd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///content/drive/MyDrive/IndicTransTokenizer\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library (from IndicTransTokenizer==0.1.3)\n",
      "  Cloning https://github.com/VarunGumma/indic_nlp_library to /tmp/pip-install-uy5pfa5p/indic-nlp-library-it2_66439cc9f000427f92b653cce9d5f5d7\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/VarunGumma/indic_nlp_library /tmp/pip-install-uy5pfa5p/indic-nlp-library-it2_66439cc9f000427f92b653cce9d5f5d7\n",
      "  Resolved https://github.com/VarunGumma/indic_nlp_library to commit 1dd6683a6dd77be3c1dbe03c226201661235c72b\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting setuptools==68.2.2 (from IndicTransTokenizer==0.1.3)\n",
      "  Downloading setuptools-68.2.2-py3-none-any.whl (807 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.9/807.9 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from IndicTransTokenizer==0.1.3) (2.2.1+cu121)\n",
      "Collecting sacremoses (from IndicTransTokenizer==0.1.3)\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from IndicTransTokenizer==0.1.3) (0.1.99)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from IndicTransTokenizer==0.1.3) (4.38.2)\n",
      "Collecting sacrebleu==2.3.1 (from IndicTransTokenizer==0.1.3)\n",
      "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting portalocker (from sacrebleu==2.3.1->IndicTransTokenizer==0.1.3)\n",
      "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu==2.3.1->IndicTransTokenizer==0.1.3) (2023.12.25)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu==2.3.1->IndicTransTokenizer==0.1.3) (0.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu==2.3.1->IndicTransTokenizer==0.1.3) (1.25.2)\n",
      "Collecting colorama (from sacrebleu==2.3.1->IndicTransTokenizer==0.1.3)\n",
      "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu==2.3.1->IndicTransTokenizer==0.1.3) (4.9.4)\n",
      "Collecting sphinx-argparse (from indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3)\n",
      "  Downloading sphinx_argparse-0.4.0-py3-none-any.whl (12 kB)\n",
      "Collecting sphinx_rtd_theme (from indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3)\n",
      "  Downloading sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl (2.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting morfessor (from indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3)\n",
      "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3) (2.0.3)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses->IndicTransTokenizer==0.1.3) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->IndicTransTokenizer==0.1.3) (1.4.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses->IndicTransTokenizer==0.1.3) (4.66.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->IndicTransTokenizer==0.1.3) (3.13.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->IndicTransTokenizer==0.1.3) (4.11.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->IndicTransTokenizer==0.1.3) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->IndicTransTokenizer==0.1.3) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->IndicTransTokenizer==0.1.3) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->IndicTransTokenizer==0.1.3) (2023.6.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->IndicTransTokenizer==0.1.3)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->IndicTransTokenizer==0.1.3)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->IndicTransTokenizer==0.1.3)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->IndicTransTokenizer==0.1.3)\n",
      "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->IndicTransTokenizer==0.1.3)\n",
      "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->IndicTransTokenizer==0.1.3)\n",
      "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->IndicTransTokenizer==0.1.3)\n",
      "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->IndicTransTokenizer==0.1.3)\n",
      "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->IndicTransTokenizer==0.1.3)\n",
      "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-nccl-cu12==2.19.3 (from torch->IndicTransTokenizer==0.1.3)\n",
      "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->IndicTransTokenizer==0.1.3)\n",
      "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->IndicTransTokenizer==0.1.3) (2.2.0)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->IndicTransTokenizer==0.1.3)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers->IndicTransTokenizer==0.1.3) (0.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->IndicTransTokenizer==0.1.3) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->IndicTransTokenizer==0.1.3) (6.0.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->IndicTransTokenizer==0.1.3) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->IndicTransTokenizer==0.1.3) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->IndicTransTokenizer==0.1.3) (0.4.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->IndicTransTokenizer==0.1.3) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3) (2023.4)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3) (2024.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->IndicTransTokenizer==0.1.3) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->IndicTransTokenizer==0.1.3) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->IndicTransTokenizer==0.1.3) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->IndicTransTokenizer==0.1.3) (2024.2.2)\n",
      "Requirement already satisfied: sphinx>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3) (5.0.2)\n",
      "Requirement already satisfied: docutils<0.21 in /usr/local/lib/python3.10/dist-packages (from sphinx_rtd_theme->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3) (0.18.1)\n",
      "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx_rtd_theme->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3)\n",
      "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->IndicTransTokenizer==0.1.3) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3) (1.16.0)\n",
      "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3) (1.0.8)\n",
      "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3) (1.0.6)\n",
      "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3) (1.0.1)\n",
      "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3) (2.0.5)\n",
      "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3) (1.1.10)\n",
      "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3) (1.0.7)\n",
      "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3) (2.16.1)\n",
      "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3) (2.2.0)\n",
      "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3) (2.14.0)\n",
      "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3) (0.7.16)\n",
      "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library-IT2@ git+https://github.com/VarunGumma/indic_nlp_library->IndicTransTokenizer==0.1.3) (1.4.1)\n",
      "Building wheels for collected packages: indic-nlp-library-IT2\n",
      "  Building wheel for indic-nlp-library-IT2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for indic-nlp-library-IT2: filename=indic_nlp_library_IT2-0.0.1-py3-none-any.whl size=56128 sha256=acd7b5abadd6ee1a0aa173cd70977a88ac84142a9803ef9896e8316a4aea24f5\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-qnot1oaa/wheels/e9/72/fa/bd9f19a3f2bacb50efcaf28b7ab89fe7ca539e35b75334befc\n",
      "Successfully built indic-nlp-library-IT2\n",
      "Installing collected packages: morfessor, setuptools, sacremoses, portalocker, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, colorama, sacrebleu, nvidia-cusparse-cu12, nvidia-cudnn-cu12, sphinxcontrib-jquery, sphinx-argparse, nvidia-cusolver-cu12, sphinx_rtd_theme, indic-nlp-library-IT2, IndicTransTokenizer\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 67.7.2\n",
      "    Uninstalling setuptools-67.7.2:\n",
      "      Successfully uninstalled setuptools-67.7.2\n",
      "  Running setup.py develop for IndicTransTokenizer\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed IndicTransTokenizer-0.1.3 colorama-0.4.6 indic-nlp-library-IT2-0.0.1 morfessor-2.0.6 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 portalocker-2.8.2 sacrebleu-2.3.1 sacremoses-0.1.1 setuptools-68.2.2 sphinx-argparse-0.4.0 sphinx_rtd_theme-2.0.0 sphinxcontrib-jquery-4.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "cd872d45c38e4941885528bf59b924da",
       "pip_warning": {
        "packages": [
         "_distutils_hack",
         "pkg_resources",
         "setuptools"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install --editable ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WKT14GNAcu6e",
    "outputId": "b1cf232f-e530-43f6-f420-43c4e16e3d97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rouge\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
      "Installing collected packages: rouge\n",
      "Successfully installed rouge-1.0.1\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install rouge\n",
    "!pip install nltk\n",
    "!pip install --upgrade nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iZwBnCJQSENE"
   },
   "source": [
    "### Importing the required files"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZL5Yzd_reUCi",
    "outputId": "3c6c0b30-86b3-4a46-bc9c-5f8094c8513c"
   },
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iNzD6K4uSENF"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from IndicTransTokenizer import IndicProcessor, IndicTransTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eU1skb7SENF"
   },
   "source": [
    "## Loding all three version of IndicTrans2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cVqZdTd-SENF"
   },
   "outputs": [],
   "source": [
    "def load_model(direction,name):\n",
    "    tokenizer = IndicTransTokenizer(direction=direction)\n",
    "    ip = IndicProcessor(inference=True)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(\"ai4bharat/indictrans2-\"+name, trust_remote_code=True)\n",
    "    return tokenizer,ip,model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "37wFQJm1SENF",
    "outputId": "642776ad-8a44-4863-95dd-2a56bca0cc7d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "en_indic_tokenizer,ip,en_indic_model = load_model(\"en-indic\",\"en-indic-dist-200M\")\n",
    "indic_en_tokenizer,ip,indic_en_model = load_model(\"indic-en\",\"indic-en-dist-200M\")\n",
    "indic_indic_tokenizer,ip,indic_indic_model = load_model(\"indic-indic\",\"indic-indic-dist-320M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQcz5ZtZSENG"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "eDbuIjBmSENG"
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    with open(path,\"r\",encoding = \"utf-8\") as file:\n",
    "        return [line.strip() for line in file]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "jiJZObg2SENG"
   },
   "outputs": [],
   "source": [
    "en_true = load_data(\"./data/test.en\")\n",
    "hi_true = load_data(\"./data/test.hi\")\n",
    "mar_true = load_data(\"./data/test.mr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G6kROb9LSENG",
    "outputId": "f4e813e3-26d1-46fd-8791-318463ade4e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2390, 2390, 2390)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(en_true),len(hi_true),len(mar_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrgJNboGSENH"
   },
   "source": [
    "## sample 1000 sentences randomly for each language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7ERtS5iaSENH"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(27)\n",
    "sample_indices = random.sample(range(len(hi_true)), 1000)\n",
    "def random_sentences(language):\n",
    "    random_sentences = [language[i] for i in sample_indices]\n",
    "    return random_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hF_M54dGSENH"
   },
   "outputs": [],
   "source": [
    "eng_1k_sentences = random_sentences(en_true)\n",
    "hi_1k_sentences =  random_sentences(hi_true)\n",
    "mar_1k_sentences = random_sentences(mar_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3lVZ9iaxSENH",
    "outputId": "91d17cae-fb99-4d22-ea4a-dae018167ac0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hi_1k_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kVrD5WBSENI"
   },
   "source": [
    "## Translation Using IndicTrans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "T7E4XnK2SENI"
   },
   "outputs": [],
   "source": [
    "def translating(source,target,tokenizer,model,sample_sentences):\n",
    "    batch = ip.preprocess_batch(sample_sentences, src_lang=source, tgt_lang=target)\n",
    "    batch = tokenizer(batch, src=True, return_tensors=\"pt\")\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(**batch, num_beams=5, num_return_sequences=1, max_length=256)\n",
    "    outputs = tokenizer.batch_decode(outputs, src=False)\n",
    "    translated_sentence = ip.postprocess_batch(outputs, lang=target)\n",
    "    return translated_sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bU6U224ySENI"
   },
   "source": [
    "### (1) Hindi to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zK6O1ecxftsH"
   },
   "outputs": [],
   "source": [
    "translated_sentence_hindi_to_eng = []\n",
    "for i in range(0,20):\n",
    "    hi_true_50 = hi_1k_sentences[i*50:(i+1)*50]\n",
    "    output  = translating(\"hin_Deva\",\"eng_Latn\",indic_en_tokenizer,indic_en_model,hi_true_50)\n",
    "    translated_sentence_hindi_to_eng.extend(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTcW2lb9SENI"
   },
   "source": [
    "#### Saving Translated Hindi to English Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_nDALZKSENI"
   },
   "outputs": [],
   "source": [
    "with open('/content/drive/MyDrive/Hindi_to_Eng.txt', 'w') as file:\n",
    "    file.write(str(translated_sentence_hindi_to_eng))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9z6cRxsaSENJ"
   },
   "source": [
    "### (2) English to Hindi translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "dcN3m17bgpAc"
   },
   "outputs": [],
   "source": [
    "translated_sentence_eng_to_hindi = []\n",
    "for i in range(0,20):\n",
    "    en_true_50 = eng_1k_sentences[i*50:(i+1)*50]\n",
    "    output = translating(\"eng_Latn\",\"hin_Deva\",en_indic_tokenizer,en_indic_model,en_true_50)\n",
    "    translated_sentence_eng_to_hindi.extend(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VV1cWfHSENJ"
   },
   "source": [
    "#### Saving Translated English to Hindi translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "OaSLo4u3k8Aw"
   },
   "outputs": [],
   "source": [
    "with open('./Translation/Eng_to_hindi.txt', 'w') as file:\n",
    "    file.write(str(translated_sentence_eng_to_hindi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cWatL4GlSENJ"
   },
   "source": [
    "### (3) Marathi to Hindi translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6yB56IYjT4g"
   },
   "outputs": [],
   "source": [
    "translated_sentence_mar_to_hindi = []\n",
    "for i in range(0,20):\n",
    "    mar_true_50 = mar_1k_sentences[i*50:(i+1)*50]\n",
    "    output = translating(\"mar_Deva\",\"hin_Deva\",indic_indic_tokenizer,indic_indic_model,mar_true_50)\n",
    "    translated_sentence_mar_to_hindi.extend(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iazpzyTsSENK"
   },
   "outputs": [],
   "source": [
    "with open('./Translation/Mar_to_hindi.txt', 'w') as file:\n",
    "    file.write(str(translated_sentence_mar_to_hindi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QC3Wzm8HSENL"
   },
   "source": [
    "### (4) Hindi to marathi translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dtp5XXZakWiA"
   },
   "outputs": [],
   "source": [
    "translated_sentence_hindi_to_mar = []\n",
    "for i in range(0,40):\n",
    "    print(i,  \"  \")\n",
    "    hi_true_50 = hi_1k_sentences[i*25:(i+1)*25]\n",
    "    output = translating(\"hin_Deva\",\"mar_Deva\",indic_indic_tokenizer,indic_indic_model,hi_true_50)\n",
    "    translated_sentence_hindi_to_mar.extend(output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ka3eR8k5SENL"
   },
   "source": [
    "### saving Translated hindi to marathi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "48_KroP6SENL"
   },
   "outputs": [],
   "source": [
    "with open('./Translation/Hindi_to_marathi.txt', 'w') as file:\n",
    "    file.write(str(translated_sentence_hindi_to_mar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading translated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DPbGvqFSXy9_"
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "def load_translated_data(path):\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "    # Read the content of the file\n",
    "        content = file.read()\n",
    "        # Parse the string representation of the list\n",
    "        sentences_list = ast.literal_eval(content)\n",
    "    return sentences_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_P6OQmfNX2Rw"
   },
   "outputs": [],
   "source": [
    "translated_sentence_eng_to_hindi = load_translated_data(\"./Translation/Eng_to_Hindi.txt\")\n",
    "translated_sentence_hindi_to_eng = load_translated_data(\"./Translation/Hindi_to_Eng.txt\")\n",
    "translated_sentence_hin_to_mar = load_translated_data(\"./Translation/Hindi_to_Mar.txt\")\n",
    "translated_sentence_mar_to_hindi = load_translated_data(\"./Translation/Mar_to_Hindi.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_aNEB4xSENL"
   },
   "source": [
    "## BLEU Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu, sentence_bleu,SmoothingFunction\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "luP0zcxLSENL"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def courpus_level_bleu_score(original,translated):\n",
    "      bleuScore_eng_to_hin=corpus_bleu([[ref] for ref in original], translated)\n",
    "      return bleuScore_eng_to_hin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "J72cvOwhSENM",
    "outputId": "17bd6f7d-0550-4d7c-b4be-76a997070862"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Level Blue Scores:\n",
      "English to hindi: 0.6926573633438512\n",
      "Hindi to English: 0.7479832197361774\n",
      "Hindi to Marathi: 0.6054683949885025\n",
      "Marathi to Hindi: 0.6086900050748287\n"
     ]
    }
   ],
   "source": [
    "eng_to_hindi_blue = courpus_level_bleu_score(hi_1k_sentences,translated_sentence_eng_to_hindi)\n",
    "hindi_to_eng_blue = courpus_level_bleu_score(eng_1k_sentences,translated_sentence_hindi_to_eng)\n",
    "hindi_to_mar_blue = courpus_level_bleu_score(mar_1k_sentences,translated_sentence_hin_to_mar)\n",
    "mar_to_hindi_blue = courpus_level_bleu_score(hi_1k_sentences,translated_sentence_mar_to_hindi)\n",
    "\n",
    "print(f\"Corpus Level Blue Scores:\\nEnglish to hindi: {eng_to_hindi_blue}\\nHindi to English: {hindi_to_eng_blue}\\nHindi to Marathi: {hindi_to_mar_blue}\\nMarathi to Hindi: {mar_to_hindi_blue}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    # Convert to lowercase\n",
    "    cleaned_sentence = sentence.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    cleaned_sentence = cleaned_sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    cleaned_sentence = ' '.join(cleaned_sentence.split())\n",
    "    \n",
    "    return cleaned_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_level_bleu_score(original, translation):\n",
    "    bleu_scores = []\n",
    "    smoothing_function = SmoothingFunction()\n",
    "    for original_sentence, translation_sentence in zip(original, translation):\n",
    "        original_sentence = clean_sentence(original_sentence)\n",
    "        translation_sentence = clean_sentence(translation_sentence)\n",
    "        original_tokens = original_sentence.split()\n",
    "        translation_tokens = translation_sentence.split()\n",
    "\n",
    "        # Calculate BLEU score\n",
    "        bleu_score = sentence_bleu([original_tokens], translation_tokens,smoothing_function=smoothing_function.method2)\n",
    "        bleu_scores.append(bleu_score)\n",
    "    \n",
    "    # Calculate average BLEU score\n",
    "    overall_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
    "    return overall_bleu_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Level Blue Scores:\n",
      "English to hindi: 0.3542139158937472\n",
      "Hindi to English: 0.4450780577789042\n",
      "Hindi to Marathi: 0.2146926372363359\n",
      "Marathi to Hindi: 0.2687288100239655\n"
     ]
    }
   ],
   "source": [
    "eng_to_hindi_blue = sentence_level_bleu_score(hi_1k_sentences,translated_sentence_eng_to_hindi)\n",
    "hindi_to_eng_blue = sentence_level_bleu_score(eng_1k_sentences,translated_sentence_hindi_to_eng)\n",
    "hindi_to_mar_blue = sentence_level_bleu_score(mar_1k_sentences,translated_sentence_hin_to_mar)\n",
    "mar_to_hindi_blue = sentence_level_bleu_score(hi_1k_sentences,translated_sentence_mar_to_hindi)\n",
    "\n",
    "print(f\"Sentence Level Blue Scores:\\nEnglish to hindi: {eng_to_hindi_blue}\\nHindi to English: {hindi_to_eng_blue}\\nHindi to Marathi: {hindi_to_mar_blue}\\nMarathi to Hindi: {mar_to_hindi_blue}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b90vlLufSENM"
   },
   "source": [
    "## ROUGE Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "lgNFDFV8SENM"
   },
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "def calculate_rouge_scores(original, translated):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(translated, original, avg=True)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sU6kcvhSENM"
   },
   "source": [
    "#### English to Hindi Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "1S4or9yWSENM",
    "outputId": "1aeab2d0-7dfb-419f-a2cc-3ba91b14d88e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Score English to Hindi:\n",
      "ROUGE_1: {'r': 0.618272792679341, 'p': 0.623610505812477, 'f': 0.6177257530350299}\n",
      "ROUGE_2: {'r': 0.3824409654525232, 'p': 0.3856898573440865, 'f': 0.38205775566133976}\n",
      "ROUGE_L: {'r': 0.5825824910050512, 'p': 0.588555397863421, 'f': 0.582574785948368}\n"
     ]
    }
   ],
   "source": [
    "rouge_eng_to_hin=calculate_rouge_scores(hi_1k_sentences,translated_sentence_eng_to_hindi)\n",
    "print(\"ROUGE Score English to Hindi:\")\n",
    "print(\"ROUGE_1:\",rouge_eng_to_hin['rouge-1'])\n",
    "print(\"ROUGE_2:\",rouge_eng_to_hin['rouge-2'])\n",
    "print(\"ROUGE_L:\",rouge_eng_to_hin['rouge-l'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVDy962WSENM"
   },
   "source": [
    "#### Hindi to English Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "BrkAW2otSENM",
    "outputId": "ad6d6b3c-2182-4e81-ddd0-92df8b3362c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Score Hindi to English:\n",
      "ROUGE_1: {'r': 0.6680526377978149, 'p': 0.6646624038681456, 'f': 0.6628035764437721}\n",
      "ROUGE_2: {'r': 0.4555270206828759, 'p': 0.448890067850876, 'f': 0.44954301315232514}\n",
      "ROUGE_L: {'r': 0.63436574860399, 'p': 0.6314363307974323, 'f': 0.6295756281285899}\n"
     ]
    }
   ],
   "source": [
    "rouge_hin_to_eng=calculate_rouge_scores(eng_1k_sentences,translated_sentence_hindi_to_eng)\n",
    "print(\"ROUGE Score Hindi to English:\")\n",
    "print(\"ROUGE_1:\",rouge_hin_to_eng['rouge-1'])\n",
    "print(\"ROUGE_2:\",rouge_hin_to_eng['rouge-2'])\n",
    "print(\"ROUGE_L:\",rouge_hin_to_eng['rouge-l'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfivsur8SENM"
   },
   "source": [
    "#### Marathi to Hindi Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "vZJjhK2fSENN",
    "outputId": "941811c2-346a-4848-c4c2-55f660d07718"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Score Marathi to Hindi:\n",
      "ROUGE_1: {'r': 0.5250729086785814, 'p': 0.5311842353709779, 'f': 0.5235051923314874}\n",
      "ROUGE_2: {'r': 0.28386135335808316, 'p': 0.28718706792869236, 'f': 0.28288370879544933}\n",
      "ROUGE_L: {'r': 0.4875967145937427, 'p': 0.4941243243945781, 'f': 0.4866813769372026}\n"
     ]
    }
   ],
   "source": [
    "rouge_mar_to_hin=calculate_rouge_scores(hi_1k_sentences,translated_sentence_mar_to_hindi)\n",
    "print(\"ROUGE Score Marathi to Hindi:\")\n",
    "print(\"ROUGE_1:\",rouge_mar_to_hin['rouge-1'])\n",
    "print(\"ROUGE_2:\",rouge_mar_to_hin['rouge-2'])\n",
    "print(\"ROUGE_L:\",rouge_mar_to_hin['rouge-l'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QOwCUNNLSENN"
   },
   "source": [
    "#### Hindi to Marathi Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "t2c30qJzSENN",
    "outputId": "7618aa14-597c-481c-844c-ef150332f1f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Score Hindi to Marathi:\n",
      "ROUGE_1: {'r': 0.4147498463812588, 'p': 0.41411767940796157, 'f': 0.41120537037461863}\n",
      "ROUGE_2: {'r': 0.18501212805608805, 'p': 0.18358572556084948, 'f': 0.1828575181355645}\n",
      "ROUGE_L: {'r': 0.38550992609229445, 'p': 0.38462095831206716, 'f': 0.38208192255912904}\n"
     ]
    }
   ],
   "source": [
    "rouge_hin_to_mar=calculate_rouge_scores(mar_1k_sentences,translated_sentence_hin_to_mar)\n",
    "print(\"ROUGE Score Hindi to Marathi:\")\n",
    "print(\"ROUGE_1:\",rouge_hin_to_mar['rouge-1'])\n",
    "print(\"ROUGE_2:\",rouge_hin_to_mar['rouge-2'])\n",
    "print(\"ROUGE_L:\",rouge_hin_to_mar['rouge-l'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ij-9agMpSENN"
   },
   "source": [
    "#                                      END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30683,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
